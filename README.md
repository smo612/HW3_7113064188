# **Epsilon-Greedy** 

## ç®—æ³•å…¬å¼

### å‹•ä½œé¸æ“‡è¦å‰‡ï¼š
$$
\displaystyle
a_t =
\begin{cases}
\text{random action from } \mathcal{A}, & \text{with probability } \varepsilon \\
\arg\max_a Q_t(a), & \text{with probability } 1 - \varepsilon
\end{cases}
$$ 

### å‹•ä½œå€¼æ›´æ–°ï¼ˆIncremental Update Formulaï¼‰ï¼š
$$
\displaystyle
Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} (R_t - Q_t(a))
$$

## ChatGPT Prompt 
```text 
You are a reinforcement learning tutor.
Please explain how the Epsilon-Greedy algorithm balances exploration and exploitation in the Multi-Armed Bandit problem.
Include an explanation of how epsilon is used to choose between random action selection and choosing the action with the highest estimated value.
Also, explain how the value estimates are updated.
```

## ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
### code generated in `Epsilon-Greedy.ipynb`  

![Epsilon-Greedy](docs/Epsilon-Greedy.png)

## çµæœè§£é‡‹  
```text 
å¾åœ–ä¸­å¯ä»¥è§€å¯Ÿåˆ°ï¼š

- **å¹³å‡å ±é…¬ï¼ˆå·¦åœ–ï¼‰** éš¨è‘—æ­¥æ•¸çš„å¢åŠ ç©©å®šä¸Šå‡ï¼ŒåˆæœŸè¿…é€Ÿçˆ¬å‡ï¼Œåœ¨ç´„ 400 æ­¥å¾Œè¶¨æ–¼å¹³ç©©ï¼Œæœ€çµ‚æ”¶æ–‚æ–¼ç´„ **1.35 å·¦å³**ã€‚é€™è¡¨ç¤º agent æˆåŠŸå­¸æœƒé¸æ“‡å ±é…¬è¼ƒé«˜çš„å‹•ä½œã€‚
- **æœ€ä½³å‹•ä½œé¸æ“‡æ¯”ä¾‹ï¼ˆå³åœ–ï¼‰** èµ·åˆç´„ç‚º 10%ï¼Œéš¨è¨“ç·´æ¬¡æ•¸æ¨é€²ç©©å®šæˆé•·ï¼Œæœ€çµ‚é”åˆ°ç´„ **80% çš„é¸æ“‡æ­£ç¢ºç‡**ï¼Œä»£è¡¨ agent å°æœ€å„ªå‹•ä½œçš„è­˜åˆ¥æ•ˆæœè‰¯å¥½ã€‚
- ä½¿ç”¨ $\varepsilon = 0.1$ æ™‚ï¼ŒEpsilon-Greedy åœ¨æ¢ç´¢èˆ‡åˆ©ç”¨ä¹‹é–“å–å¾—è‰¯å¥½å¹³è¡¡ï¼Œä¿ç•™äº†è¶³å¤ æ¢ç´¢æ©Ÿæœƒä¾†é¿å…éæ—©æ”¶æ–‚ï¼ŒåŒæ™‚èƒ½ç©©å®šåœ°å­¸ç¿’æœ‰æ•ˆç­–ç•¥ã€‚
- æœ¬æ¼”ç®—æ³•åœ¨æ¯å€‹ time step åƒ…éœ€ $O(1)$ çš„æ›´æ–°èˆ‡é¸æ“‡æ“ä½œï¼Œç©ºé–“ä¸Šåªéœ€å„²å­˜æ¯å€‹å‹•ä½œçš„ $Q(a)$ å’Œ $N(a)$ï¼Œé©åˆæ‡‰ç”¨æ–¼å¤§é‡é‡è¤‡è©¦é©—çš„å­¸ç¿’ä»»å‹™ã€‚
```


# **UCB (Upper Confidence Bound)** 

## ç®—æ³•å…¬å¼

### å‹•ä½œé¸æ“‡è¦å‰‡ï¼š
$$
\displaystyle
a_t = \arg\max_a \left[ Q_t(a) + c \cdot \sqrt{ \frac{ \ln t }{ N_t(a) + \epsilon } } \right]
$$

### å‹•ä½œå€¼æ›´æ–°ï¼ˆIncremental Update Formulaï¼‰ï¼š
$$
\displaystyle
Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} (R_t - Q_t(a))
$$

## ChatGPT Prompt 
```text 
You are a reinforcement learning tutor.
Please explain the Upper Confidence Bound (UCB) algorithm used in the Multi-Armed Bandit problem.
Describe how it balances exploration and exploitation by adjusting the confidence term based on the number of times an action has been selected.
Explain how the constant c affects exploration, and how the action selection formula evolves as more steps are taken.
```

## ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
### code generated in `UCB.ipynb` 

![UCB](docs/UCB.png)

## çµæœè§£é‡‹
```text
å¾åœ–ä¸­å¯ä»¥è§€å¯Ÿåˆ°ï¼š

- **å¹³å‡å ±é…¬ï¼ˆå·¦åœ–ï¼‰** éš¨è‘—æ­¥æ•¸å¢åŠ ç©©å®šä¸Šå‡ï¼ŒåˆæœŸè¿…é€Ÿçˆ¬å‡ï¼Œç´„åœ¨ç¬¬ 300 æ­¥å¾Œè¶¨æ–¼å¹³ç©©ï¼Œæœ€çµ‚æ”¶æ–‚æ–¼ç´„ **1.45ï½1.5 å·¦å³**ã€‚é€™é¡¯ç¤º agent æˆåŠŸå­¸æœƒé›†ä¸­é¸æ“‡å ±é…¬è¼ƒé«˜çš„å‹•ä½œã€‚
- **æœ€ä½³å‹•ä½œé¸æ“‡æ¯”ä¾‹ï¼ˆå³åœ–ï¼‰** åˆæœŸç´„ç‚º 10%ï¼Œè¿…é€Ÿæˆé•·ä¸¦æœ€çµ‚ç©©å®šæ–¼ **85%ï½88%**ï¼Œä»£è¡¨ agent å°æœ€å„ªå‹•ä½œçš„è¾¨è­˜èˆ‡åŸ·è¡Œæ•ˆæœè‰¯å¥½ã€‚
- UCB ç„¡éœ€é å…ˆè¨­å®šæ¢ç´¢ç‡ï¼Œé€éå‹•æ…‹èª¿æ•´çš„ confidence term é”æˆæ¢ç´¢èˆ‡åˆ©ç”¨çš„å¹³è¡¡ï¼Œé¿å…é™·å…¥å›ºå®šç­–ç•¥çš„å±€é™ã€‚
- å¸¸æ•¸ $c=2$ æä¾›é©ç•¶æ¢ç´¢å£“åŠ›ï¼Œè‹¥ $c$ èª¿æ•´ä¸ç•¶æœƒå°è‡´éåº¦æ¢ç´¢æˆ–æ—©æœŸæ”¶æ–‚ã€‚
- æœ¬æ¼”ç®—æ³•åœ¨æ¯å€‹ time step åƒ…éœ€ $O(1)$ çš„æ›´æ–°èˆ‡é¸æ“‡æ“ä½œï¼Œç©ºé–“ä¸Šåªéœ€å„²å­˜æ¯å€‹å‹•ä½œçš„ $Q(a)$ èˆ‡ $N(a)$ï¼Œé©åˆæ‡‰ç”¨æ–¼å¤§é‡é‡è¤‡è©¦é©—çš„å­¸ç¿’ä»»å‹™ã€‚
```

# **Softmax** 

## ç®—æ³•å…¬å¼

### å‹•ä½œé¸æ“‡æ©Ÿç‡ï¼š
$$
\displaystyle
P(a) = \frac{ \exp\left( \frac{Q_t(a)}{\tau} \right) }{ \sum_b \exp\left( \frac{Q_t(b)}{\tau} \right) }
$$

### å‹•ä½œå€¼æ›´æ–°ï¼ˆIncremental Update Formulaï¼‰ï¼š
$$
\displaystyle
Q_{t+1}(a) = Q_t(a) + \frac{1}{N_t(a)} (R_t - Q_t(a))
$$

## ChatGPT Prompt 
```text 
You are a reinforcement learning tutor.
Please explain the Softmax (Boltzmann exploration) algorithm used in the Multi-Armed Bandit problem.
Describe how the temperature parameter Ï„ controls the balance between exploration and exploitation, and how the action selection probability is calculated using estimated Q values.
Also, explain how different values of Ï„ affect learning performance.
```

## ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
### code generated in `Softmax.ipynb` 

![Softmax](docs/Softmax.png)

## çµæœè§£é‡‹
```text
å¾åœ–ä¸­å¯ä»¥è§€å¯Ÿåˆ°ï¼š

- **å¹³å‡å ±é…¬ï¼ˆå·¦åœ–ï¼‰** ç©©å®šä¸Šå‡ï¼ŒåˆæœŸå­¸ç¿’é€Ÿåº¦è‰¯å¥½ï¼Œä½†æœ€çµ‚æ”¶æ–‚æ–¼ç´„ **1.25ï½1.3**ï¼Œç•¥ä½æ–¼ UCB èˆ‡ Epsilon-Greedyï¼Œä»£è¡¨é¸æ“‡ä»è¼ƒåˆ†æ•£ã€‚
- **æœ€ä½³å‹•ä½œé¸æ“‡æ¯”ä¾‹ï¼ˆå³åœ–ï¼‰** æœ€çµ‚ç´„ç‚º **55%ï½58%**ï¼Œé¡¯ç¤º agent ä¸¦æœªæŒçºŒèšç„¦æ–¼æœ€å„ªé¸é …ï¼Œç¬¦åˆ Softmax æ¢ç´¢æ€§å¼·çš„ç‰¹æ€§ã€‚
- Softmax åˆ©ç”¨æº«åº¦åƒæ•¸ $\tau$ èª¿ç¯€æ¢ç´¢ç¨‹åº¦ï¼Œæœ¬å¯¦é©—ä¸­ $\tau = 0.1$ï¼Œé›–éæ¥µé«˜ï¼Œä½†ä»ä¿ç•™ä¸€å®šç¨‹åº¦çš„éš¨æ©Ÿæ€§ï¼Œå°è‡´æœ€ä½³å‹•ä½œé¸æ“‡ç‡ç•¥ä½ã€‚
- ç›¸è¼ƒæ–¼ Epsilon-Greedy èˆ‡ UCB çš„ç­–ç•¥åå‘ exploitationï¼ŒSoftmax ä¿æœ‰æ©Ÿç‡æ€§æ¢ç´¢ï¼Œé©åˆæ–¼ä¸ç¢ºå®šæ€§é«˜æˆ–ééœæ…‹å ±é…¬çš„å ´æ™¯ã€‚
- æ­¤æ¼”ç®—æ³•è¨ˆç®—æ¯å€‹å‹•ä½œæ©Ÿç‡éœ€é€²è¡Œ softmaxï¼Œè¨ˆç®—æˆæœ¬ç•¥é«˜æ–¼è²ªå©ªç­–ç•¥ï¼Œä½†ä¾ç„¶èƒ½ä»¥ $O(k)$ å®Œæˆï¼Œé©ç”¨æ–¼ä¸­ç­‰è¦æ¨¡è¡Œå‹•ç©ºé–“ã€‚
```

# **Thompson Sampling**

## ç®—æ³•å…¬å¼

### å°æ–¼æ¯ä¸€å€‹å‹•ä½œ ğ‘ï¼ŒThompson Sampling çš„æ­¥é©Ÿå¦‚ä¸‹ï¼š
### 1. ç‚ºæ¯å€‹å‹•ä½œç¶­è­·ä¸€å€‹æ©Ÿç‡åˆ†å¸ƒï¼ˆä¾‹å¦‚ï¼šè²ä»–åˆ†å¸ƒï¼‰
### 2. å¾è©²åˆ†å¸ƒä¸­éš¨æ©ŸæŠ½æ¨£ä¸€å€‹å€¼ï¼š

$$
\displaystyle
\theta_a \sim \text{Beta}(\alpha_a, \beta_a)
$$

### 3. é¸æ“‡å…·æœ‰æœ€å¤§æŠ½æ¨£å€¼çš„å‹•ä½œï¼š

$$
\displaystyle
a_t = \arg\max_a \theta_a
$$

## ChatGPT Prompt 
```text 
You are a reinforcement learning tutor.
Please explain the Thompson Sampling algorithm used in the Multi-Armed Bandit problem.
Describe how the algorithm maintains a posterior distribution for each action and selects actions by sampling from these distributions.
Explain how the algorithm naturally balances exploration and exploitation through probabilistic sampling, and how it updates its belief using reward feedback.
```

## ç¨‹å¼ç¢¼èˆ‡åœ–è¡¨
### code generated in `Thompson Sampling.ipynb` 

![Thompson Sampling](docs/ThompsonSampling.png)

### çµæœè§£é‡‹
```text
å¾åœ–ä¸­å¯ä»¥è§€å¯Ÿåˆ°ï¼š

- **å¹³å‡å ±é…¬ï¼ˆå·¦åœ–ï¼‰** éš¨æ™‚é–“ç©©å®šä¸Šå‡ï¼Œç´„åœ¨ 300 æ­¥å¾Œè¶¨æ–¼å¹³ç·©ï¼Œæœ€çµ‚æ”¶æ–‚æ–¼ **0.9 å·¦å³**ï¼Œç¬¦åˆ Bernoulli ç’°å¢ƒçš„æœ€å¤§å¯èƒ½æœŸæœ›ã€‚
- **æœ€ä½³å‹•ä½œé¸æ“‡æ¯”ä¾‹ï¼ˆå³åœ–ï¼‰** èµ·å§‹ç´„ç‚º 10%ï¼Œè¿…é€Ÿæå‡ä¸¦åœ¨ç´„ 1000 æ­¥å¾Œç©©å®šæ–¼ **87ï½89%**ï¼Œä»£è¡¨ agent èƒ½æˆåŠŸè¾¨è­˜ä¸¦åå¥½é¸æ“‡æœ€ä½³å‹•ä½œã€‚
- Thompson Sampling é€éå°æ¯å€‹å‹•ä½œå»ºç«‹å¾Œé©—åˆ†å¸ƒï¼Œæ ¹æ“šæ©Ÿç‡æŠ½æ¨£é€²è¡Œé¸æ“‡ï¼Œåœ¨å­¸ç¿’åˆæœŸå…·æœ‰é«˜åº¦æ¢ç´¢æ€§ï¼Œè€Œå¾Œé€æ¼¸å°ˆæ³¨æ–¼å ±é…¬è¼ƒä½³çš„é¸é …ã€‚
- ç›¸è¼ƒæ–¼ Îµ-Greedy æˆ– UCBï¼ŒThompson Sampling ä¸éœ€è¦é¡¯å¼çš„è¶…åƒæ•¸èª¿ç¯€ï¼ˆå¦‚ Îµ æˆ– cï¼‰ï¼Œè€Œæ˜¯ä¾æ“šçå‹µæ•¸æ“šå‹•æ…‹èª¿æ•´ï¼Œå…·æœ‰æ›´å¼·çš„è‡ªé©æ‡‰æ€§ã€‚
- æ­¤æ¼”ç®—æ³•æ¯ä¸€æ­¥ä»¥ $O(k)$ æŠ½æ¨£èˆ‡æ¯”è¼ƒæ–¹å¼é¸å–å‹•ä½œï¼Œä¸”åƒ…éœ€ç¶­è­·æ¯å€‹å‹•ä½œçš„æˆåŠŸ/å¤±æ•—è¨ˆæ•¸ï¼Œéå¸¸é©åˆè™•ç† Bernoulli å‹ bandit å•é¡Œã€‚
```
